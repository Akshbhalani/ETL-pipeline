{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "48c2a886-8223-4872-bf88-5e2cc0cdf8b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook upload_file_in_s3_testing.ipynb to script\n",
      "[NbConvertApp] Writing 3300 bytes to upload_file_in_s3_testing.py\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --to script upload_file_in_s3_testing.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d0d433fe-bcea-4294-a761-080d0ef5dc10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import pandas as pd\n",
    "import boto3\n",
    "from io import StringIO\n",
    "import json\n",
    "from sqlalchemy import create_engine\n",
    "import urllib.parse\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8c0980b6-61ef-4d83-a157-d7e0dda26c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from configparser import ConfigParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "004b2f1d-4e90-4488-b489-c2f52d6e83f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "a58372e7-aeae-489b-a82b-820428638267",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the credential file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "060a98e8-b151-4025-b32a-bbf9faa25d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def credentialFile(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        return json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "52d3c874-7f50-4acc-8d5d-6ed1b3ebee44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the data from the s3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "8faa32e1-7fee-4a2a-a02b-d89ad18a69f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv_from_s3(bucket_name, file_key, credentials):\n",
    "    try:\n",
    "        # Create an S3 client\n",
    "        s3 = boto3.client('s3',\n",
    "                          aws_access_key_id=credentials['aws_access_key_id'],\n",
    "                          aws_secret_access_key=credentials['aws_secret_access_key'])\n",
    "        \n",
    "        # Read the CSV file from S3\n",
    "        response = s3.get_object(Bucket=bucket_name, Key=file_key)\n",
    "        csv_content = response['Body'].read().decode('utf-8')\n",
    "        \n",
    "        # Convert the CSV content to a DataFrame\n",
    "        data = pd.read_csv(StringIO(csv_content))\n",
    "        \n",
    "        return data\n",
    "    \n",
    "    except s3.exceptions.NoSuchKey:\n",
    "        logging.error(f\"The file {file_key} does not exist in the bucket {bucket_name}.\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An error occurred: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "9494f16a-4704-4679-99e7-81ffa6e458be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dim table defination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "fe7f5af8-5cfa-4e18-9b52-613d2d29ef50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dim_table(data, id_column, dim_columns, key_column):\n",
    "    dim_table = data[[id_column] + dim_columns].drop_duplicates(subset=[id_column]).reset_index(drop=True)\n",
    "    dim_table[key_column] = dim_table.index + 1\n",
    "    return dim_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "9e77fcda-0de6-4802-a2d4-ac452a755394",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fact table defination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "8726a032-b934-4d2b-95d5-871cf7759d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_fact_table(data, fact_columns):\n",
    "    fact_table = data[fact_columns]\n",
    "    return fact_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "0c625780-0e59-4356-a77f-d6649a32012f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_to_s3(dataframe, bucket_name, ETL_file_key, credentials):\n",
    "    try:\n",
    "        # Create an S3 client\n",
    "        s3 = boto3.client('s3',\n",
    "                          aws_access_key_id=credentials['aws_access_key_id'],\n",
    "                          aws_secret_access_key=credentials['aws_secret_access_key'])\n",
    "\n",
    "        # Convert the DataFrame to a CSV string\n",
    "        csv_buffer = StringIO()\n",
    "        dataframe.to_csv(csv_buffer, index=False)\n",
    "\n",
    "        # Upload the CSV string to S3\n",
    "        s3.put_object(Bucket=bucket_name, Key=ETL_file_key, Body=csv_buffer.getvalue())\n",
    "\n",
    "        logging.info(f\"Uploaded {ETL_file_key} to S3 bucket {bucket_name} successfully.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to upload {ETL_file_key} to S3: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e69718c-fe89-4a41-967c-14f9911002b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "    bucket_name = 'bucket_name'\n",
    "    file_key = 'file_name'\n",
    "    credentials_file = 'credential_file_name'\n",
    "\n",
    "\n",
    "    # Load credentials\n",
    "    credentials = credentialFile(credentials_file)\n",
    "    \n",
    "    try:\n",
    "        # calling of reading the data from s3\n",
    "        data = read_csv_from_s3(bucket_name, file_key, credentials)\n",
    "\n",
    "        # defining dim fact table schema\n",
    "        if data is not None:\n",
    "            dim_specs = {\n",
    "                'product': {\n",
    "                    'id_column': 'Product_ID',\n",
    "                    'dim_columns': ['Product_Name', 'Category', 'Sub_Category'],\n",
    "                    'key_column': 'ProductKey'\n",
    "                },\n",
    "                'customer': {\n",
    "                    'id_column': 'Customer_ID',\n",
    "                    'dim_columns': ['Customer_Name', 'Segment'],\n",
    "                    'key_column': 'CustomerKey'\n",
    "                },\n",
    "                'location': {\n",
    "                    'id_column': 'Postal_Code',\n",
    "                    'dim_columns': ['Country', 'City', 'State', 'Region'],\n",
    "                    'key_column': 'LocationKey'\n",
    "                },\n",
    "                'date': {\n",
    "                    'id_column': 'Order_Date',\n",
    "                    'dim_columns': [ 'Ship_Date', 'Ship_Mode'],\n",
    "                    'key_column': 'DateKey'\n",
    "                }\n",
    "            }\n",
    "\n",
    "            fact_columns = ['Order_ID', 'Customer_ID', 'Product_ID', 'Postal_Code', 'Order_Date', 'Ship_Date', 'Sales']\n",
    "\n",
    "\n",
    "            # dim tables calling\n",
    "            dim_tables = {}\n",
    "            for dim_name, dim_spec in dim_specs.items():\n",
    "                dim_table = create_dim_table(data, dim_spec['id_column'], dim_spec['dim_columns'], dim_spec['key_column'])\n",
    "                merge_columns = [dim_spec['id_column']] + dim_spec['dim_columns']\n",
    "                dim_tables[dim_name] = (dim_table, dim_spec['key_column'], merge_columns)\n",
    "                logging.info(f\"{dim_name.capitalize()} Dimension Table:\")\n",
    "                \n",
    "            print(dim_tables)\n",
    "\n",
    "            \n",
    "            # fact table calling\n",
    "            fact_table = create_fact_table(data, fact_columns)\n",
    "            logging.info(\"Fact Table:\")\n",
    "\n",
    "\n",
    "            upload_keys = {\n",
    "                'product': 'product_file_name',\n",
    "                'customer': 'customer_file_name',\n",
    "                'date': 'date_file_name',\n",
    "                'location': 'location_file_name',\n",
    "                'fact': 'fact_file_name'\n",
    "            }\n",
    "\n",
    "            # Upload dimension tables to S3\n",
    "            for dim_name, dim_info in dim_tables.items():\n",
    "                dim_table = dim_info[0]\n",
    "                ETL_file_key = upload_keys[dim_name]\n",
    "                upload_to_s3(dim_table, bucket_name, ETL_file_key, credentials)\n",
    "        \n",
    "            # Upload fact table to S3\n",
    "            upload_to_s3(fact_table, bucket_name, upload_keys['fact'], credentials)\n",
    "\n",
    "    \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to read CSV from S3: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "dc35e085-0eba-4e1d-b09c-ce9ccd03177a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Product Dimension Table:\n",
      "INFO:root:Customer Dimension Table:\n",
      "INFO:root:Location Dimension Table:\n",
      "INFO:root:Date Dimension Table:\n",
      "INFO:root:Fact Table:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'product': (           Product_ID                                       Product_Name  \\\n",
      "0     FUR-BO-10001798                  Bush Somerset Collection Bookcase   \n",
      "1     FUR-CH-10000454  Hon Deluxe Fabric Upholstered Stacking Chairs,...   \n",
      "2     OFF-LA-10000240  Self-Adhesive Address Labels for Typewriters b...   \n",
      "3     FUR-TA-10000577      Bretford CR4500 Series Slim Rectangular Table   \n",
      "4     OFF-ST-10000760                      Eldon Fold N Roll Cart System   \n",
      "...               ...                                                ...   \n",
      "1856  TEC-AC-10002380       Sony 8GB Class 10 Micro SDHC R40 Memory Card   \n",
      "1857  TEC-PH-10002817                    RCA ViSYS 25425RE1 Corded phone   \n",
      "1858  TEC-MA-10003589                       Cisco 8961 IP Phone Charcoal   \n",
      "1859  OFF-AP-10003099                        Eureka Hand Vacuum, Bagless   \n",
      "1860  TEC-PH-10002645                                              LG G2   \n",
      "\n",
      "             Category Sub_Category  ProductKey  \n",
      "0           Furniture    Bookcases           1  \n",
      "1           Furniture       Chairs           2  \n",
      "2     Office Supplies       Labels           3  \n",
      "3           Furniture       Tables           4  \n",
      "4     Office Supplies      Storage           5  \n",
      "...               ...          ...         ...  \n",
      "1856       Technology  Accessories        1857  \n",
      "1857       Technology       Phones        1858  \n",
      "1858       Technology     Machines        1859  \n",
      "1859  Office Supplies   Appliances        1860  \n",
      "1860       Technology       Phones        1861  \n",
      "\n",
      "[1861 rows x 5 columns], 'ProductKey', ['Product_ID', 'Product_Name', 'Category', 'Sub_Category']), 'customer': (    Customer_ID      Customer_Name      Segment  CustomerKey\n",
      "0      CG-12520        Claire Gute     Consumer            1\n",
      "1      DV-13045    Darrin Van Huff    Corporate            2\n",
      "2      SO-20335      Sean O Donnel     Consumer            3\n",
      "3      BH-11710    Brosina Hoffman     Consumer            4\n",
      "4      AA-10480       Andrew Allen     Consumer            5\n",
      "..          ...                ...          ...          ...\n",
      "788    CJ-11875       Carl Jackson    Corporate          789\n",
      "789    RS-19870         Roy Skaria  Home Office          790\n",
      "790    SC-20845         Sung Chung     Consumer          791\n",
      "791    RE-19405    Ricardo Emerson     Consumer          792\n",
      "792    SM-20905  Susan MacKendrick     Consumer          793\n",
      "\n",
      "[793 rows x 4 columns], 'CustomerKey', ['Customer_ID', 'Customer_Name', 'Segment']), 'location': (     Postal_Code        Country             City           State   Region  \\\n",
      "0          42420  United States        Henderson        Kentucky    South   \n",
      "1          90036  United States      Los Angeles      California     West   \n",
      "2          33311  United States  Fort Lauderdale         Florida    South   \n",
      "3          90032  United States      Los Angeles      California     West   \n",
      "4          28027  United States          Concord  North Carolina    South   \n",
      "..           ...            ...              ...             ...      ...   \n",
      "622        55433  United States      Coon Rapids       Minnesota  Central   \n",
      "623        92672  United States     San Clemente      California     West   \n",
      "624        94568  United States           Dublin      California     West   \n",
      "625        93405  United States  San Luis Obispo      California     West   \n",
      "626        72762  United States       Springdale        Arkansas    South   \n",
      "\n",
      "     LocationKey  \n",
      "0              1  \n",
      "1              2  \n",
      "2              3  \n",
      "3              4  \n",
      "4              5  \n",
      "..           ...  \n",
      "622          623  \n",
      "623          624  \n",
      "624          625  \n",
      "625          626  \n",
      "626          627  \n",
      "\n",
      "[627 rows x 6 columns], 'LocationKey', ['Postal_Code', 'Country', 'City', 'State', 'Region']), 'date': (      Order_Date   Ship_Date       Ship_Mode  DateKey\n",
      "0     2017-11-08  2017-11-11    Second Class        1\n",
      "1     2017-06-12  2017-06-16    Second Class        2\n",
      "2     2016-10-11  2016-10-18  Standard Class        3\n",
      "3     2015-06-09  2015-06-14  Standard Class        4\n",
      "4     2018-04-15  2018-04-20  Standard Class        5\n",
      "...          ...         ...             ...      ...\n",
      "1225  2015-06-10  2015-06-15    Second Class     1226\n",
      "1226  2017-10-11  2017-10-15  Standard Class     1227\n",
      "1227  2015-06-18  2015-06-23  Standard Class     1228\n",
      "1228  2018-02-28  2018-03-06  Standard Class     1229\n",
      "1229  2016-05-09  2016-05-13  Standard Class     1230\n",
      "\n",
      "[1230 rows x 4 columns], 'DateKey', ['Order_Date', 'Ship_Date', 'Ship_Mode'])}\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8565c80-7e9c-4b7a-af0b-45e1508a7741",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e402ec4f-1f7e-4197-a619-0db349e5ebf4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
