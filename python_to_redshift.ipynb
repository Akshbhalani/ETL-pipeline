{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0fef65c7-5f35-4882-a405-936744c7dac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import pandas as pd\n",
    "import boto3\n",
    "from io import StringIO\n",
    "import json\n",
    "from sqlalchemy import create_engine\n",
    "import urllib.parse\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fc864a05-26b3-4d8b-9e58-59d5ce0209ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5c7ff84b-6c60-435d-993a-e751b6f70c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def credentialFile(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        return json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3afb4a6d-a883-4d72-a786-a0fd9e8a65a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv_from_s3(bucket_name, ETL_file_key, credentials):\n",
    "    try:\n",
    "        # Create an S3 client\n",
    "        s3 = boto3.client('s3',\n",
    "                          aws_access_key_id=credentials['aws_access_key_id'],\n",
    "                          aws_secret_access_key=credentials['aws_secret_access_key'])\n",
    "\n",
    "        # Read the CSV file from S3\n",
    "        response = s3.get_object(Bucket=bucket_name, Key=ETL_file_key)\n",
    "        csv_content = response['Body'].read().decode('utf-8')\n",
    "\n",
    "        # Convert the CSV content to a DataFrame\n",
    "        data = pd.read_csv(StringIO(csv_content))\n",
    "\n",
    "        logging.info(f\"File {ETL_file_key} read from S3 bucket {bucket_name} successfully.\")\n",
    "        return data\n",
    "\n",
    "    except s3.exceptions.NoSuchKey:\n",
    "        logging.error(f\"The file {file_key} does not exist in the bucket {bucket_name}.\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An error occurred while reading {file_key} from S3: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a910f92d-4556-417e-ac92-f207d092fc2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_to_redshift(table_name, file_key, redshift_credentials, credentials, bucket_name):\n",
    "    try:\n",
    "        conn = psycopg2.connect(\n",
    "            dbname=redshift_credentials['db'],\n",
    "            user=redshift_credentials['user'],\n",
    "            password=redshift_credentials['password'],\n",
    "            host=redshift_credentials['host'],\n",
    "            port=redshift_credentials['port']\n",
    "        )\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        copy_query = f\"\"\"\n",
    "        COPY {table_name}\n",
    "        FROM 's3://{bucket_name}/{file_key}'\n",
    "        iam_role 'iam_role_link'\n",
    "        CSV  \n",
    "        IGNOREHEADER 1;\n",
    "        \"\"\"\n",
    "\n",
    "        cursor.execute(copy_query)\n",
    "        conn.commit()\n",
    "\n",
    "        logging.info(f\"Copied data from S3 to Redshift table {table_name} successfully.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to copy data to Redshift table {table_name}: {e}\")\n",
    "        raise\n",
    "\n",
    "    finally: \n",
    "        cursor.close()\n",
    "        conn.close()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fb911240-282e-4f40-9c6a-f35073062db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    bucket_name = 'bucket_name'\n",
    "    credentials_file = 'credential_file_name'\n",
    "    redshift_credentials_file = 'redshift_credential_file_name'\n",
    "\n",
    "\n",
    "    # Load credentials\n",
    "    credentials = credentialFile(credentials_file)\n",
    "    redshift_credentials = credentialFile(redshift_credentials_file)\n",
    "\n",
    "    # File keys for the uploaded dimension and fact tables\n",
    "    file_keys = {\n",
    "        'product': 'product_file_name',\n",
    "        'customer': 'customer_file_name',\n",
    "        'date': 'date_file_name',\n",
    "        'location': 'location_file_name',\n",
    "        'fact': 'fact_file_name'\n",
    "    }\n",
    "\n",
    "    try:    \n",
    "        # Read dimension and fact tables from S3\n",
    "        dataframes = {}\n",
    "        for table_name, file_key in file_keys.items():\n",
    "            df = read_csv_from_s3(bucket_name, file_key, credentials)\n",
    "            dataframes[table_name] = df\n",
    "\n",
    "            # Copy each dataframe to Redshift\n",
    "            copy_to_redshift(table_name, file_key, redshift_credentials, credentials, bucket_name)\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to read data: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "27435cf1-72d6-4188-9db2-e71e0f4ed2f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:File product_dimension.csv read from S3 bucket fileuplodtesting successfully.\n",
      "INFO:root:Copied data from S3 to Redshift table product successfully.\n",
      "INFO:root:File customer_dimension.csv read from S3 bucket fileuplodtesting successfully.\n",
      "INFO:root:Copied data from S3 to Redshift table customer successfully.\n",
      "INFO:root:File date_dimension.csv read from S3 bucket fileuplodtesting successfully.\n",
      "INFO:root:Copied data from S3 to Redshift table date successfully.\n",
      "INFO:root:File location_dimension.csv read from S3 bucket fileuplodtesting successfully.\n",
      "INFO:root:Copied data from S3 to Redshift table location successfully.\n",
      "INFO:root:File fact_table.csv read from S3 bucket fileuplodtesting successfully.\n",
      "INFO:root:Copied data from S3 to Redshift table fact successfully.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a8822d-2a15-40f9-a426-e5845dfdbb7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7b0514-20c6-43b2-87cd-f8a77ebf2d0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
